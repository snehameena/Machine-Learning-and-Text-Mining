{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Surname Classification Using a Character RNN (Week 8)\n",
    "\n",
    "In this example, we see surname classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "import os\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary, Vectorizer, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\"Class to process text and extract vocabulary for mapping\"\"\"\n",
    "\n",
    "    def __init__(self, token_to_idx=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            token_to_idx (dict): a pre-existing map of tokens to indices\n",
    "             \n",
    "        _token_to_idx:\n",
    "           {'<MASK>': 0, '<UNK>': 1, '<BEGIN>': 2, '<END>': 3, 'T': 4, 'o': 5, 't': 6, 'a': 7, ....., 'Á': 79}\n",
    "        _idx_to_token: \n",
    "           {0:'<MASK>'0, 1:'<UNK>', 2:'<BEGIN>',   3:'<END>',   4:'T',  5:'o',  6:'t',  7:'a',....., 79:'Á'}\n",
    "            \n",
    "        _token_to_idx: {'Arabic': 0, 'Chinese': 1, ..., 'Vietnamese': 17}\n",
    "        _idx_to_token: {0:'Arabic',  1:'Chinese', ...,  17:'Vietnamese'}\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx   \n",
    "\n",
    "        self._idx_to_token = {idx: token \n",
    "                              for token, idx in self._token_to_idx.items()}\n",
    "        \n",
    "    def to_serializable(self):\n",
    "        \"\"\" returns a dictionary that can be serialized \"\"\"\n",
    "        return {'token_to_idx': self._token_to_idx}\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        \"\"\" instantiates the Vocabulary from a serialized dictionary \"\"\"\n",
    "        return cls(**contents)\n",
    "\n",
    "    def add_token(self, token):\n",
    "        \"\"\"Update mapping dicts based on the token.\n",
    "\n",
    "        Args:\n",
    "            token (str): the item to add into the Vocabulary\n",
    "        Returns:\n",
    "            index (int): the integer corresponding to the token\n",
    "        \"\"\"\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "            \n",
    "    def add_many(self, tokens):\n",
    "        \"\"\"Add a list of tokens into the Vocabulary\n",
    "        \n",
    "        Args:\n",
    "            tokens (list): a list of string tokens\n",
    "        Returns:\n",
    "            indices (list): a list of indices corresponding to the tokens\n",
    "        \"\"\"\n",
    "        return [self.add_token(token) for token in tokens]\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"Retrieve the index associated with the token \n",
    "        \n",
    "        Args:\n",
    "            token (str): the token to look up \n",
    "        Returns:\n",
    "            index (int): the index corresponding to the token\n",
    "        \"\"\"\n",
    "        return self._token_to_idx[token]\n",
    "\n",
    "    def lookup_index(self, index):\n",
    "        \"\"\"Return the token associated with the index\n",
    "        \n",
    "        Args: \n",
    "            index (int): the index to look up\n",
    "        Returns:\n",
    "            token (str): the token corresponding to the index\n",
    "        Raises:\n",
    "            KeyError: if the index is not in the Vocabulary\n",
    "        \"\"\"\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self._idx_to_token[index]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceVocabulary(Vocabulary):\n",
    "    def __init__(self, token_to_idx=None, unk_token=\"<UNK>\",\n",
    "                 mask_token=\"<MASK>\", begin_seq_token=\"<BEGIN>\",\n",
    "                 end_seq_token=\"<END>\"):\n",
    "\n",
    "        super(SequenceVocabulary, self).__init__(token_to_idx)\n",
    "\n",
    "        self._mask_token = mask_token\n",
    "        self._unk_token = unk_token\n",
    "        self._begin_seq_token = begin_seq_token\n",
    "        self._end_seq_token = end_seq_token\n",
    "\n",
    "        self.mask_index = self.add_token(self._mask_token)           # mask_index is 0\n",
    "        self.unk_index = self.add_token(self._unk_token)             # unk_index is 1\n",
    "        self.begin_seq_index = self.add_token(self._begin_seq_token) # begin_seq_index is 2\n",
    "        self.end_seq_index = self.add_token(self._end_seq_token)     # end_seq_index is 3\n",
    "\n",
    "    def to_serializable(self):\n",
    "        contents = super(SequenceVocabulary, self).to_serializable()\n",
    "        contents.update({'unk_token': self._unk_token,\n",
    "                         'mask_token': self._mask_token,\n",
    "                         'begin_seq_token': self._begin_seq_token,\n",
    "                         'end_seq_token': self._end_seq_token})\n",
    "        return contents\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"Retrieve the index associated with the token \n",
    "          or the UNK index if token isn't present.\n",
    "        \n",
    "        Args:\n",
    "            token (str): the token to look up \n",
    "        Returns:\n",
    "            index (int): the index corresponding to the token\n",
    "        Notes:\n",
    "            `unk_index` needs to be >=0 (having been added into the Vocabulary) \n",
    "              for the UNK functionality \n",
    "        \"\"\"\n",
    "        if self.unk_index >= 0:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurnameVectorizer(object):\n",
    "    \"\"\" The Vectorizer which coordinates the Vocabularies and puts them to use\"\"\"   \n",
    "    def __init__(self, char_vocab, nationality_vocab):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            char_vocab (Vocabulary): maps characters to integers\n",
    "            nationality_vocab (Vocabulary): maps nationalities to integers\n",
    "        \"\"\"\n",
    "        self.char_vocab = char_vocab\n",
    "        self.nationality_vocab = nationality_vocab\n",
    "\n",
    "    def vectorize(self, surname, vector_length=-1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            surname (str): the string of characters\n",
    "            vector_length (int): an argument for forcing the length of index vector\n",
    "        \"\"\"\n",
    "        indices = [self.char_vocab.begin_seq_index]\n",
    "        indices.extend(self.char_vocab.lookup_token(token) \n",
    "                       for token in surname)\n",
    "        indices.append(self.char_vocab.end_seq_index)\n",
    "\n",
    "        if vector_length < 0:\n",
    "            vector_length = len(indices)\n",
    "\n",
    "        out_vector = np.zeros(vector_length, dtype=np.int64)         \n",
    "        out_vector[:len(indices)] = indices\n",
    "        out_vector[len(indices):] = self.char_vocab.mask_index    \n",
    "                                                                  \n",
    "        \"\"\"    \n",
    "        mask_index is 0\n",
    "        unk_index is 1\n",
    "        begin_seq_index is 2\n",
    "        end_seq_index is 3\n",
    "        \n",
    "        When surname is \"McMahan\", M=5,c=6, a=7, h=8, and n=9 \n",
    "        \n",
    "        out_vector = [2, 5, 6, 5, 7, 8, 7, 9, 3, 0, 0, 0, ..., 0]\n",
    "        len(indices) = 9\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        return out_vector, len(indices)\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataframe(cls, surname_df):\n",
    "        \"\"\"Instantiate the vectorizer from the dataset dataframe\n",
    "        \n",
    "        Args:\n",
    "            surname_df (pandas.DataFrame): the surnames dataset\n",
    "        Returns:\n",
    "            an instance of the SurnameVectorizer\n",
    "        \"\"\"\n",
    "        char_vocab = SequenceVocabulary()\n",
    "        nationality_vocab = Vocabulary()\n",
    "\n",
    "        for index, row in surname_df.iterrows():\n",
    "            for char in row.surname:\n",
    "                char_vocab.add_token(char)   # case sensitive\n",
    "            nationality_vocab.add_token(row.nationality)\n",
    "\n",
    "        return cls(char_vocab, nationality_vocab)\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        char_vocab = SequenceVocabulary.from_serializable(contents['char_vocab'])\n",
    "        nat_vocab =  Vocabulary.from_serializable(contents['nationality_vocab'])\n",
    "\n",
    "        return cls(char_vocab=char_vocab, nationality_vocab=nat_vocab)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        return {'char_vocab': self.char_vocab.to_serializable(), \n",
    "                'nationality_vocab': self.nationality_vocab.to_serializable()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurnameDataset(Dataset):\n",
    "    def __init__(self, surname_df, vectorizer):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            surname_df (pandas.DataFrame): the dataset\n",
    "            vectorizer (SurnameVectorizer): vectorizer instatiated from dataset\n",
    "        \"\"\"\n",
    "        self.surname_df = surname_df \n",
    "        self._vectorizer = vectorizer\n",
    "\n",
    "        self._max_seq_length = max(map(len, self.surname_df.surname)) + 2   # add 2 for begin_seq_token and end_seq_token\n",
    "\n",
    "        self.train_df = self.surname_df[self.surname_df.split=='train']\n",
    "        self.train_size = len(self.train_df)\n",
    "\n",
    "        self.val_df = self.surname_df[self.surname_df.split=='val']\n",
    "        self.validation_size = len(self.val_df)\n",
    "\n",
    "        self.test_df = self.surname_df[self.surname_df.split=='test']\n",
    "        self.test_size = len(self.test_df)\n",
    "\n",
    "        self._lookup_dict = {'train': (self.train_df, self.train_size), \n",
    "                             'val': (self.val_df, self.validation_size), \n",
    "                             'test': (self.test_df, self.test_size)}\n",
    "\n",
    "        self.set_split('train')\n",
    "        \n",
    "        # Class weights\n",
    "        class_counts = self.train_df.nationality.value_counts().to_dict()   # {'English': 2972, 'Russian': 2373, ....}\n",
    "        def sort_key(item):\n",
    "            return self._vectorizer.nationality_vocab.lookup_token(item[0]) # e.g, index of English is 4\n",
    "        sorted_counts = sorted(class_counts.items(), key=sort_key)          # sort by the index number of nationality_vocab\n",
    "                                # {('Arabic', 1603), ('Chinese', 220), ('Czech', 414), ('Dutch', 236),('English', 2972), ...}\n",
    "        \n",
    "        frequencies = [count for _, count in sorted_counts]\n",
    "        self.class_weights = 1.0 / torch.tensor(frequencies, dtype=torch.float32)\n",
    "\n",
    "        \n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, surname_csv):\n",
    "        \"\"\"Load dataset and make a new vectorizer from scratch\n",
    "        \n",
    "        Args:\n",
    "            surname_csv (str): location of the dataset\n",
    "        Returns:\n",
    "            an instance of SurnameDataset\n",
    "        \"\"\"\n",
    "        surname_df = pd.read_csv(surname_csv)\n",
    "        train_surname_df = surname_df[surname_df.split=='train']\n",
    "        return cls(surname_df, SurnameVectorizer.from_dataframe(train_surname_df))\n",
    "        \n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls, surname_csv, vectorizer_filepath):\n",
    "        \"\"\"Load dataset and the corresponding vectorizer. \n",
    "        Used in the case in the vectorizer has been cached for re-use\n",
    "        \n",
    "        Args:\n",
    "            surname_csv (str): location of the dataset\n",
    "            vectorizer_filepath (str): location of the saved vectorizer\n",
    "        Returns:\n",
    "            an instance of SurnameDataset\n",
    "        \"\"\"\n",
    "        surname_df = pd.read_csv(surname_csv)\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
    "        return cls(surname_df, vectorizer)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        \"\"\"a static method for loading the vectorizer from file\n",
    "        \n",
    "        Args:\n",
    "            vectorizer_filepath (str): the location of the serialized vectorizer\n",
    "        Returns:\n",
    "            an instance of SurnameVectorizer\n",
    "        \"\"\"\n",
    "        with open(vectorizer_filepath) as fp:\n",
    "            return SurnameVectorizer.from_serializable(json.load(fp))\n",
    "\n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "        \"\"\"saves the vectorizer to disk using json\n",
    "        \n",
    "        Args:\n",
    "            vectorizer_filepath (str): the location to save the vectorizer\n",
    "        \"\"\"\n",
    "        with open(vectorizer_filepath, \"w\") as fp:\n",
    "            json.dump(self._vectorizer.to_serializable(), fp)\n",
    "\n",
    "    def get_vectorizer(self):\n",
    "        \"\"\" returns the vectorizer \"\"\"\n",
    "        return self._vectorizer\n",
    "\n",
    "    def set_split(self, split=\"train\"):\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"the primary entry point method for PyTorch datasets\n",
    "        \n",
    "        Args:\n",
    "            index (int): the index to the data point \n",
    "        Returns:\n",
    "            a dictionary holding the data point's:\n",
    "                features (x_data)\n",
    "                label (y_target)\n",
    "                feature length (x_length)\n",
    "        \"\"\"\n",
    "        row = self._target_df.iloc[index]\n",
    "        \n",
    "        surname_vector, vec_length = \\\n",
    "            self._vectorizer.vectorize(row.surname, self._max_seq_length)\n",
    "        \n",
    "        nationality_index = \\\n",
    "            self._vectorizer.nationality_vocab.lookup_token(row.nationality)\n",
    "\n",
    "        return {'x_data': surname_vector, \n",
    "                'y_target': nationality_index, \n",
    "                'x_length': vec_length}\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        \"\"\"Given a batch size, return the number of batches in the dataset\n",
    "        \n",
    "        Args:\n",
    "            batch_size (int)\n",
    "        Returns:\n",
    "            number of batches in the dataset\n",
    "        \"\"\"\n",
    "        return len(self) // batch_size\n",
    "\n",
    "    \n",
    "\n",
    "def generate_batches(dataset, batch_size, shuffle=True,\n",
    "                     drop_last=True, device=\"cpu\"): \n",
    "    \"\"\"\n",
    "    A generator function which wraps the PyTorch DataLoader. It will \n",
    "      ensure each tensor is on the write device location.\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def column_gather(y_out, x_lengths):\n",
    "    '''Get a specific vector from each batch datapoint in `y_out`.\n",
    "\n",
    "    More precisely, iterate over batch row indices, get the vector that's at\n",
    "    the position indicated by the corresponding value in `x_lengths` at the row\n",
    "    index.\n",
    "\n",
    "    y_out gets the last hidden vector of each input: (batch, hidden_size)\n",
    "\n",
    "    Args:\n",
    "        y_out (torch.FloatTensor, torch.cuda.FloatTensor)\n",
    "            shape: (batch, sequence, feature)\n",
    "        x_lengths (torch.LongTensor, torch.cuda.LongTensor)\n",
    "            shape: (batch,)\n",
    "\n",
    "    Returns:\n",
    "        y_out (torch.FloatTensor, torch.cuda.FloatTensor)\n",
    "            shape: (batch, feature)\n",
    "    '''\n",
    "    x_lengths = x_lengths.long().detach().cpu().numpy() - 1\n",
    "\n",
    "    out = []  \n",
    "    for batch_index, column_index in enumerate(x_lengths): # out gets the last hidden vector of each input: (batch, hidden_size)\n",
    "        out.append(y_out[batch_index, column_index])\n",
    "\n",
    "    return torch.stack(out)\n",
    "\n",
    "\n",
    "class ElmanRNN(nn.Module):\n",
    "    \"\"\" an Elman RNN built using the RNNCell \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, batch_first=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_size (int): size of the input vectors\n",
    "            hidden_size (int): size of the hidden state vectors\n",
    "            batch_first (bool): whether the 0th dimension is batch\n",
    "        \"\"\"\n",
    "        super(ElmanRNN, self).__init__()\n",
    "        \n",
    "        self.rnn_cell = nn.RNNCell(input_size, hidden_size)\n",
    "        \n",
    "        self.batch_first = batch_first\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def _initial_hidden(self, batch_size):\n",
    "        return torch.zeros((batch_size, self.hidden_size))\n",
    "\n",
    "    def forward(self, x_in, initial_hidden=None):\n",
    "        \"\"\"The forward pass of the ElmanRNN\n",
    "        \n",
    "        Args:\n",
    "            x_in (torch.Tensor): an input data tensor. \n",
    "                If self.batch_first: x_in.shape = (batch, seq_size, feat_size)\n",
    "                Else: x_in.shape = (seq_size, batch, feat_size)\n",
    "            initial_hidden (torch.Tensor): the initial hidden state for the RNN\n",
    "        Returns:\n",
    "            hiddens (torch.Tensor): The outputs of the RNN at each time step. \n",
    "                If self.batch_first: hiddens.shape = (batch, seq_size, hidden_size)\n",
    "                Else: hiddens.shape = (seq_size, batch, hidden_size)\n",
    "        \"\"\"\n",
    "        if self.batch_first:\n",
    "            batch_size, seq_size, feat_size = x_in.size()\n",
    "            x_in = x_in.permute(1, 0, 2)\n",
    "        else:\n",
    "            seq_size, batch_size, feat_size = x_in.size()\n",
    "    \n",
    "        hiddens = []\n",
    "\n",
    "        if initial_hidden is None:\n",
    "            initial_hidden = self._initial_hidden(batch_size)\n",
    "            initial_hidden = initial_hidden.to(x_in.device)\n",
    "\n",
    "        hidden_t = initial_hidden\n",
    "                    \n",
    "        for t in range(seq_size):\n",
    "            hidden_t = self.rnn_cell(x_in[t], hidden_t)  # x_in[t]: (batch, feat_size) of t seq; hidden_t: (batch, hidden_size)\n",
    "            hiddens.append(hidden_t)\n",
    "            \n",
    "        hiddens = torch.stack(hiddens)\n",
    "\n",
    "        if self.batch_first:\n",
    "            hiddens = hiddens.permute(1, 0, 2)\n",
    "\n",
    "        return hiddens\n",
    "\n",
    "\n",
    "\n",
    "class SurnameClassifier(nn.Module):\n",
    "    \"\"\" A Classifier with an RNN to extract features and an MLP to classify \"\"\"\n",
    "    def __init__(self, embedding_size, num_embeddings, num_classes,\n",
    "                 rnn_hidden_size, batch_first=True, padding_idx=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embedding_size (int): The size of the character embeddings\n",
    "            num_embeddings (int): The number of characters to embed\n",
    "            num_classes (int): The size of the prediction vector \n",
    "                Note: the number of nationalities\n",
    "            rnn_hidden_size (int): The size of the RNN's hidden state\n",
    "            batch_first (bool): Informs whether the input tensors will \n",
    "                have batch or the sequence on the 0th dimension\n",
    "            padding_idx (int): The index for the tensor padding; \n",
    "                see torch.nn.Embedding\n",
    "        \"\"\"\n",
    "        super(SurnameClassifier, self).__init__()\n",
    "\n",
    "        self.emb = nn.Embedding(num_embeddings=num_embeddings,\n",
    "                                embedding_dim=embedding_size,\n",
    "                                padding_idx=padding_idx)\n",
    "        \n",
    "        self.rnn = ElmanRNN(input_size=embedding_size,\n",
    "                             hidden_size=rnn_hidden_size,\n",
    "                             batch_first=batch_first)\n",
    "               \n",
    "        self.fc1 = nn.Linear(in_features=rnn_hidden_size,\n",
    "                         out_features=rnn_hidden_size)\n",
    "        self.fc2 = nn.Linear(in_features=rnn_hidden_size,\n",
    "                          out_features=num_classes)\n",
    "        # for batch norm\n",
    "        self.bn1 = nn.BatchNorm1d(rnn_hidden_size) \n",
    "\n",
    "\n",
    "    def forward(self, x_in, x_lengths=None, apply_softmax=False):\n",
    "        \"\"\"The forward pass of the classifier\n",
    "        \n",
    "        Args:\n",
    "            x_in (torch.Tensor): an input data tensor. \n",
    "                x_in.shape should be (batch, input_dim)\n",
    "            x_lengths (torch.Tensor): the lengths of each sequence in the batch.\n",
    "                They are used to find the final vector of each sequence\n",
    "            apply_softmax (bool): a flag for the softmax activation\n",
    "                should be false if used with the Cross Entropy losses\n",
    "        Returns:\n",
    "            the resulting tensor. tensor.shape should be (batch, output_dim)\n",
    "        \"\"\"\n",
    "        x_embedded = self.emb(x_in)\n",
    "        y_out = self.rnn(x_embedded)  # x_embedded: (batch, seq_size, feat_size); y_out: (batch, seq_size, hidden_size)\n",
    "\n",
    "        if x_lengths is not None:\n",
    "            y_out = column_gather(y_out, x_lengths) # y_out gets the last hidden vector of each input: (batch, hidden_size)\n",
    "                                                    # i.e, the last hidden vector of last character, not including padding\n",
    "        else:\n",
    "            y_out = y_out[:, -1, :]                 # y_out gets the last hidden vector of each input: (batch, hidden_size)\n",
    "       \n",
    "        # for batch norm and change dropout\n",
    "        y_out = F.relu(self.fc1(F.dropout(y_out, 0.5, training=self.training)))  # y_out: (batch, hidden_size)  \n",
    "        #y_out = F.relu(self.bn1(self.fc1(F.dropout(y_out, 0.5, training=self.training))))\n",
    "\n",
    "        \n",
    "        # for change dropout\n",
    "        y_out = self.fc2(F.dropout(y_out, 0.5, training=self.training))          # y_out: (batch, num_classes)\n",
    "\n",
    "        if apply_softmax:\n",
    "            y_out = F.softmax(y_out, dim=1)\n",
    "\n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed_everywhere(seed, cuda):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def handle_dirs(dirpath):\n",
    "    if not os.path.exists(dirpath):\n",
    "        os.makedirs(dirpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: False\n"
     ]
    }
   ],
   "source": [
    "args = Namespace(\n",
    "    # Data and path information\n",
    "    surname_csv=\"data/surnames/surnames_with_splits.csv\",\n",
    "    vectorizer_file=\"vectorizer.json\",\n",
    "    model_state_file=\"model.pth\",\n",
    "    save_dir=\"model_storage/ch6/surname_classification\",\n",
    "    # Model hyper parameter\n",
    "    char_embedding_size=100,\n",
    "    rnn_hidden_size=64,\n",
    "    # Training hyper parameter\n",
    "    num_epochs=100,\n",
    "    #num_epochs=200,\n",
    "    learning_rate=1e-3,\n",
    "    batch_size=64,\n",
    "    seed=1337,\n",
    "    early_stopping_criteria=5,\n",
    "    # Runtime hyper parameter\n",
    "    cuda=True,\n",
    "    catch_keyboard_interrupt=True,\n",
    "    reload_from_files=False,\n",
    "    expand_filepaths_to_save_dir=True,\n",
    ")\n",
    "\n",
    "# Check CUDA\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "    \n",
    "print(\"Using CUDA: {}\".format(args.cuda))\n",
    "\n",
    "\n",
    "if args.expand_filepaths_to_save_dir:\n",
    "    args.vectorizer_file = os.path.join(args.save_dir,\n",
    "                                        args.vectorizer_file)\n",
    "\n",
    "    args.model_state_file = os.path.join(args.save_dir,\n",
    "                                         args.model_state_file)\n",
    "    \n",
    "# Set seed for reproducibility\n",
    "set_seed_everywhere(args.seed, args.cuda)\n",
    "\n",
    "# handle dirs\n",
    "handle_dirs(args.save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0,
     4
    ]
   },
   "outputs": [],
   "source": [
    "if args.reload_from_files and os.path.exists(args.vectorizer_file):\n",
    "    # training from a checkpoint\n",
    "    dataset = SurnameDataset.load_dataset_and_load_vectorizer(args.surname_csv, \n",
    "                                                              args.vectorizer_file)\n",
    "else:\n",
    "    # create dataset and vectorizer\n",
    "    dataset = SurnameDataset.load_dataset_and_make_vectorizer(args.surname_csv)\n",
    "    dataset.save_vectorizer(args.vectorizer_file)\n",
    "\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "\n",
    "classifier = SurnameClassifier(embedding_size=args.char_embedding_size, \n",
    "                               num_embeddings=len(vectorizer.char_vocab),\n",
    "                               num_classes=len(vectorizer.nationality_vocab),\n",
    "                               rnn_hidden_size=args.rnn_hidden_size,\n",
    "                               padding_idx=vectorizer.char_vocab.mask_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<MASK>': 0,\n",
       " '<UNK>': 1,\n",
       " '<BEGIN>': 2,\n",
       " '<END>': 3,\n",
       " 'T': 4,\n",
       " 'o': 5,\n",
       " 't': 6,\n",
       " 'a': 7,\n",
       " 'h': 8,\n",
       " 'A': 9,\n",
       " 'b': 10,\n",
       " 'u': 11,\n",
       " 'd': 12,\n",
       " 'F': 13,\n",
       " 'k': 14,\n",
       " 'r': 15,\n",
       " 'y': 16,\n",
       " 'S': 17,\n",
       " 'e': 18,\n",
       " 'g': 19,\n",
       " 'C': 20,\n",
       " 'm': 21,\n",
       " 'H': 22,\n",
       " 'i': 23,\n",
       " 'K': 24,\n",
       " 'n': 25,\n",
       " 'W': 26,\n",
       " 's': 27,\n",
       " 'f': 28,\n",
       " 'G': 29,\n",
       " 'M': 30,\n",
       " 'l': 31,\n",
       " 'B': 32,\n",
       " 'z': 33,\n",
       " 'N': 34,\n",
       " 'I': 35,\n",
       " 'w': 36,\n",
       " 'D': 37,\n",
       " 'Q': 38,\n",
       " 'j': 39,\n",
       " 'E': 40,\n",
       " 'R': 41,\n",
       " 'Z': 42,\n",
       " 'c': 43,\n",
       " 'Y': 44,\n",
       " 'J': 45,\n",
       " 'L': 46,\n",
       " 'O': 47,\n",
       " '-': 48,\n",
       " 'P': 49,\n",
       " 'X': 50,\n",
       " 'p': 51,\n",
       " ':': 52,\n",
       " 'v': 53,\n",
       " 'U': 54,\n",
       " '1': 55,\n",
       " 'V': 56,\n",
       " 'x': 57,\n",
       " 'q': 58,\n",
       " 'é': 59,\n",
       " 'É': 60,\n",
       " \"'\": 61,\n",
       " 'ß': 62,\n",
       " 'ö': 63,\n",
       " 'ä': 64,\n",
       " 'ü': 65,\n",
       " 'ú': 66,\n",
       " 'à': 67,\n",
       " 'ò': 68,\n",
       " 'è': 69,\n",
       " 'ó': 70,\n",
       " 'Ś': 71,\n",
       " 'ą': 72,\n",
       " 'ń': 73,\n",
       " 'á': 74,\n",
       " 'ż': 75,\n",
       " 'õ': 76,\n",
       " 'í': 77,\n",
       " 'ñ': 78,\n",
       " 'Á': 79}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.char_vocab._token_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Arabic': 0,\n",
       " 'Chinese': 1,\n",
       " 'Czech': 2,\n",
       " 'Dutch': 3,\n",
       " 'English': 4,\n",
       " 'French': 5,\n",
       " 'German': 6,\n",
       " 'Greek': 7,\n",
       " 'Irish': 8,\n",
       " 'Italian': 9,\n",
       " 'Japanese': 10,\n",
       " 'Korean': 11,\n",
       " 'Polish': 12,\n",
       " 'Portuguese': 13,\n",
       " 'Russian': 14,\n",
       " 'Scottish': 15,\n",
       " 'Spanish': 16,\n",
       " 'Vietnamese': 17}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.nationality_vocab._token_to_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def make_train_state(args):\n",
    "    return {'stop_early': False,\n",
    "            'early_stopping_step': 0,\n",
    "            'early_stopping_best_val': 1e8,\n",
    "            'learning_rate': args.learning_rate,\n",
    "            'epoch_index': 0,\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'test_loss': -1,\n",
    "            'test_acc': -1,\n",
    "            'model_filename': args.model_state_file}\n",
    "\n",
    "\n",
    "def update_train_state(args, model, train_state):\n",
    "    \"\"\"Handle the training state updates.\n",
    "\n",
    "    Components:\n",
    "     - Early Stopping: Prevent overfitting.\n",
    "     - Model Checkpoint: Model is saved if the model is better\n",
    "    \n",
    "    :param args: main arguments\n",
    "    :param model: model to train\n",
    "    :param train_state: a dictionary representing the training state values\n",
    "    :returns:\n",
    "        a new train_state\n",
    "    \"\"\"\n",
    "\n",
    "    # Save one model at least\n",
    "    if train_state['epoch_index'] == 0:\n",
    "        torch.save(model.state_dict(), train_state['model_filename'])\n",
    "        train_state['stop_early'] = False\n",
    "\n",
    "    # Save model if performance improved\n",
    "    elif train_state['epoch_index'] >= 1:\n",
    "        loss_tm1, loss_t = train_state['val_loss'][-2:]\n",
    "         \n",
    "        # If loss worsened\n",
    "        if loss_t >= loss_tm1:\n",
    "            # Update step\n",
    "            train_state['early_stopping_step'] += 1\n",
    "        # Loss decreased\n",
    "        else:\n",
    "            # Save the best model\n",
    "            if loss_t < train_state['early_stopping_best_val']:\n",
    "                torch.save(model.state_dict(), train_state['model_filename'])\n",
    "                train_state['early_stopping_best_val'] = loss_t\n",
    "\n",
    "            # Reset early stopping step\n",
    "            train_state['early_stopping_step'] = 0\n",
    "\n",
    "        # Stop early ?\n",
    "        train_state['stop_early'] = \\\n",
    "            train_state['early_stopping_step'] >= args.early_stopping_criteria\n",
    "\n",
    "    return train_state\n",
    "\n",
    "\n",
    "def compute_accuracy(y_pred, y_target):\n",
    "    _, y_pred_indices = y_pred.max(dim=1)\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return n_correct / len(y_pred_indices) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dfd078bed2848e8ac79b6280951937d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='training routine', style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a980ba1244b4edda827a6b0c2bbeee8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='split=train', max=120.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f00d2afa975f4a3bb0a1f91c07472314",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='split=val', max=25.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classifier = classifier.to(args.device)\n",
    "dataset.class_weights = dataset.class_weights.to(args.device)\n",
    "    \n",
    "loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                           mode='min', factor=0.5,\n",
    "                                           patience=1)\n",
    "\n",
    "train_state = make_train_state(args)\n",
    "\n",
    "epoch_bar = tqdm(desc='training routine', total=args.num_epochs, position=0)\n",
    "\n",
    "dataset.set_split('train')\n",
    "train_bar = tqdm(desc='split=train', total=dataset.get_num_batches(args.batch_size), position=1, leave=True)\n",
    "\n",
    "dataset.set_split('val')\n",
    "val_bar = tqdm(desc='split=val', total=dataset.get_num_batches(args.batch_size), position=1, leave=True)\n",
    "\n",
    "t = time.localtime()\n",
    "start_time = time.strftime(\"%H:%M:%S\", t)\n",
    "print(\"strt time##\",start_time)\n",
    "try:\n",
    "    for epoch_index in range(args.num_epochs):\n",
    "        train_state['epoch_index'] = epoch_index\n",
    "\n",
    "        # Iterate over training dataset\n",
    "\n",
    "        # setup: batch generator, set loss and acc to 0, set train mode on\n",
    "        dataset.set_split('train')\n",
    "        batch_generator = generate_batches(dataset, \n",
    "                                           batch_size=args.batch_size, \n",
    "                                           device=args.device)\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        classifier.train()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            # the training routine is these 5 steps:\n",
    "\n",
    "            # --------------------------------------    \n",
    "            # step 1. zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # step 2. compute the output\n",
    "            y_pred = classifier(x_in=batch_dict['x_data'], \n",
    "                                x_lengths=batch_dict['x_length'])\n",
    "\n",
    "            # step 3. compute the loss\n",
    "            loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "    \n",
    "            running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
    "\n",
    "            # step 4. use loss to produce gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # step 5. use optimizer to take gradient step\n",
    "            optimizer.step()\n",
    "            # -----------------------------------------\n",
    "            # compute the accuracy\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "            # update bar\n",
    "            train_bar.set_postfix(loss=running_loss, acc=running_acc, epoch=epoch_index)\n",
    "            train_bar.update()\n",
    "\n",
    "        train_state['train_loss'].append(running_loss)\n",
    "        train_state['train_acc'].append(running_acc)\n",
    "\n",
    "        # Iterate over val dataset\n",
    "\n",
    "        # setup: batch generator, set loss and acc to 0; set eval mode on\n",
    "\n",
    "        dataset.set_split('val')\n",
    "        batch_generator = generate_batches(dataset, \n",
    "                                           batch_size=args.batch_size, \n",
    "                                           device=args.device)\n",
    "        running_loss = 0.\n",
    "        running_acc = 0.\n",
    "        classifier.eval()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            # compute the output\n",
    "            y_pred = classifier(x_in=batch_dict['x_data'], \n",
    "                                x_lengths=batch_dict['x_length'])\n",
    "\n",
    "            # step 3. compute the loss\n",
    "            loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "            running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
    "\n",
    "            # compute the accuracy\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "            val_bar.set_postfix(loss=running_loss, acc=running_acc, epoch=epoch_index)\n",
    "            val_bar.update()\n",
    "\n",
    "        train_state['val_loss'].append(running_loss)\n",
    "        train_state['val_acc'].append(running_acc)\n",
    "\n",
    "        train_state = update_train_state(args=args, model=classifier, \n",
    "                                         train_state=train_state)\n",
    "\n",
    "        scheduler.step(train_state['val_loss'][-1])\n",
    "\n",
    "        train_bar.n = 0\n",
    "        val_bar.n = 0\n",
    "        epoch_bar.update()\n",
    "\n",
    "        if train_state['stop_early']:\n",
    "            break\n",
    "            \n",
    "except KeyboardInterrupt:\n",
    "    print(\"Exiting loop\")\n",
    "t1 = time.localtime()\n",
    "start_time = time.strftime(\"%H:%M:%S\", t1)\n",
    "print(\"strt time##\",start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = train_state['train_acc']\n",
    "val_acc = train_state['val_acc']\n",
    "loss = train_state['train_loss']\n",
    "val_loss = train_state['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()   # clear figure\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the loss & accuracy on the test set using the best available model\n",
    "\n",
    "classifier.load_state_dict(torch.load(train_state['model_filename']))\n",
    "\n",
    "classifier = classifier.to(args.device)\n",
    "dataset.class_weights = dataset.class_weights.to(args.device)\n",
    "loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n",
    "\n",
    "dataset.set_split('test')\n",
    "batch_generator = generate_batches(dataset, \n",
    "                                   batch_size=args.batch_size, \n",
    "                                   device=args.device)\n",
    "running_loss = 0.\n",
    "running_acc = 0.\n",
    "classifier.eval()\n",
    "\n",
    "y_pred_list = []         # store predicted values for confusion matrix\n",
    "y_nationality_list = []  # ground truth value\n",
    "\n",
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    # compute the output\n",
    "    y_pred =  classifier(batch_dict['x_data'],\n",
    "                         x_lengths=batch_dict['x_length'])\n",
    "\n",
    "    # store predicted values and ground truth values for calculating confusion matrix\n",
    "    y_pred_list.extend(y_pred.max(dim=1)[1].cpu().numpy())\n",
    "    y_nationality_list.extend(batch_dict['y_target'].cpu().numpy())\n",
    "    \n",
    "    # compute the loss\n",
    "    loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "    loss_t = loss.item()\n",
    "    running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "    # compute the accuracy\n",
    "    acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "    running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "train_state['test_loss'] = running_loss\n",
    "train_state['test_acc'] = running_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.6494850254058837;\n",
      "Test Accuracy: 50.625\n"
     ]
    }
   ],
   "source": [
    "print(\"Test loss: {};\".format(train_state['test_loss']))\n",
    "print(\"Test Accuracy: {}\".format(train_state['test_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Arabic', 'Chinese', 'Czech', 'Dutch', 'English', 'French', 'German', 'Greek', 'Irish', 'Italian', 'Japanese', 'Korean', 'Polish', 'Portuguese', 'Russian', 'Scottish', 'Spanish', 'Vietnamese']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "nationality_classes = []\n",
    "for i in range(len(dataset._vectorizer.nationality_vocab)):\n",
    "    nationality_classes.append(dataset._vectorizer.nationality_vocab.lookup_index(i))\n",
    "print(nationality_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True        Arabic  Chinese  Czech  Dutch  English  French  German  Greek  \\\n",
      "Predicted                                                                   \n",
      "Arabic         215        0      2      1       16       0       4      0   \n",
      "Chinese          0       17      1      1        8       1       1      0   \n",
      "Czech            0        0     12      0       17       0       3      0   \n",
      "Dutch            0        0      1     15       51       4      13      0   \n",
      "English          0        0      1      3       35       0       1      0   \n",
      "French           0        0      3      4       78      16       9      1   \n",
      "German           0        0      6      4       48       3      39      0   \n",
      "Greek            5        0      0      0        6       1       0     17   \n",
      "Irish            0        0      3      0       75       2       8      0   \n",
      "Italian          0        0      1      0        1       0       1      0   \n",
      "Japanese         6        0      3      0        3       0       1      1   \n",
      "Korean           2       11      0      1        2       0       0      0   \n",
      "Polish           0        0     16      0        2       0       0      0   \n",
      "Portuguese       0        0      0      0        3       2       0      2   \n",
      "Russian          0        0      8      0        9       2       2      0   \n",
      "Scottish         0        0      3      1       78       3       2      0   \n",
      "Spanish          0        0      3      1        3       0       0      0   \n",
      "Vietnamese       2        3      0      1        1       0       0      0   \n",
      "\n",
      "True        Irish  Italian  Japanese  Korean  Polish  Portuguese  Russian  \\\n",
      "Predicted                                                                   \n",
      "Arabic          0        0         4       0       0           0        4   \n",
      "Chinese         0        0         0       5       0           0        2   \n",
      "Czech           2        1         0       0       2           0       14   \n",
      "Dutch           0        0         0       1       0           0        3   \n",
      "English         0        0         0       0       0           0        6   \n",
      "French          2        4         1       0       0           0       11   \n",
      "German          4        2         0       0       1           1        8   \n",
      "Greek           0        1         0       0       0           0        4   \n",
      "Irish          14        2         0       0       1           0        8   \n",
      "Italian         0       56        10       0       0           2        5   \n",
      "Japanese        0        2        91       0       1           0        9   \n",
      "Korean          1        0         2       4       1           0        2   \n",
      "Polish          0        1         0       0       8           0       11   \n",
      "Portuguese      0        7         0       0       0           2        2   \n",
      "Russian         1        0         2       0       2           0      248   \n",
      "Scottish        2        0         0       0       0           0        4   \n",
      "Spanish         0       11         0       0       1           3        3   \n",
      "Vietnamese      0        1         2       3       0           0        0   \n",
      "\n",
      "True        Scottish  Spanish  Vietnamese  \n",
      "Predicted                                  \n",
      "Arabic             0        0           0  \n",
      "Chinese            1        0           5  \n",
      "Czech              1        1           0  \n",
      "Dutch              0        1           0  \n",
      "English            1        1           0  \n",
      "French             1        1           0  \n",
      "German             2        1           0  \n",
      "Greek              0        4           0  \n",
      "Irish              0        1           1  \n",
      "Italian            0        6           1  \n",
      "Japanese           0        0           0  \n",
      "Korean             0        0           2  \n",
      "Polish             0        0           0  \n",
      "Portuguese         0        8           0  \n",
      "Russian            0        0           0  \n",
      "Scottish           5        1           0  \n",
      "Spanish            0       15           0  \n",
      "Vietnamese         0        0           1  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "cm = confusion_matrix(y_nationality_list, y_pred_list)\n",
    "cm_df = pd.DataFrame(cm.T, index=nationality_classes, columns=nationality_classes)\n",
    "cm_df.index.name = 'Predicted'\n",
    "cm_df.columns.name = 'True'\n",
    "print(cm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.93      0.90       230\n",
      "           1       0.40      0.55      0.47        31\n",
      "           2       0.23      0.19      0.21        63\n",
      "           3       0.17      0.47      0.25        32\n",
      "           4       0.73      0.08      0.14       436\n",
      "           5       0.12      0.47      0.19        34\n",
      "           6       0.33      0.46      0.38        84\n",
      "           7       0.45      0.81      0.58        21\n",
      "           8       0.12      0.54      0.20        26\n",
      "           9       0.67      0.64      0.65        88\n",
      "          10       0.78      0.81      0.79       112\n",
      "          11       0.14      0.31      0.20        13\n",
      "          12       0.21      0.47      0.29        17\n",
      "          13       0.08      0.25      0.12         8\n",
      "          14       0.91      0.72      0.80       344\n",
      "          15       0.05      0.45      0.09        11\n",
      "          16       0.38      0.38      0.38        40\n",
      "          17       0.07      0.10      0.08        10\n",
      "\n",
      "    accuracy                           0.51      1600\n",
      "   macro avg       0.37      0.48      0.37      1600\n",
      "weighted avg       0.67      0.51      0.51      1600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_nationality_list, y_pred_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_nationality(surname, classifier, vectorizer):\n",
    "    vectorized_surname, vec_length = vectorizer.vectorize(surname)\n",
    "    vectorized_surname = torch.tensor(vectorized_surname).unsqueeze(dim=0)\n",
    "    vec_length = torch.tensor([vec_length], dtype=torch.int64)\n",
    "    \n",
    "    result = classifier(vectorized_surname, vec_length, apply_softmax=True)\n",
    "    probability_values, indices = result.max(dim=1)\n",
    "    \n",
    "    index = indices.item()\n",
    "    prob_value = probability_values.item()\n",
    "\n",
    "    predicted_nationality = vectorizer.nationality_vocab.lookup_index(index)\n",
    "\n",
    "    return {'nationality': predicted_nationality, 'probability': prob_value, 'surname': surname}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'nationality': 'Irish', 'probability': 0.6904856562614441, 'surname': 'McMahan'}\n",
      "{'nationality': 'Japanese', 'probability': 0.951169490814209, 'surname': 'Nakamoto'}\n",
      "{'nationality': 'Chinese', 'probability': 0.47007274627685547, 'surname': 'Wan'}\n",
      "{'nationality': 'Korean', 'probability': 0.4307156801223755, 'surname': 'Cho'}\n"
     ]
    }
   ],
   "source": [
    "# surname = input(\"Enter a surname: \")\n",
    "classifier = classifier.to(\"cpu\")\n",
    "for surname in ['McMahan', 'Nakamoto', 'Wan', 'Cho']:\n",
    "    print(predict_nationality(surname, classifier, vectorizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise:\n",
    "1. Change the value of dropout (0.5) to other values\n",
    "2. Try out batch norm with dropout\n",
    "3. Try to explore other hyperparameters, such as hidden_dim and batch_size.\n",
    "4. Instead of an Elman RNN built using the RNNCell, use built-in RNNs: nn.RNN, nn.LSTM, and nn.GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Assignment #3: due on October 19, 2020 in Singapore Time\n",
    "\n",
    "- #### Instead of using an Elman RNN, build the best model (based on test loss and test accuracy) using built-in RNNs (nn.RNN, nn.LSTM, and nn.GRU) by exploring following parameters in RNNs (nn.RNN, nn.LSTM, and nn.GRU):\n",
    "  1. input_size  \n",
    "  2. hidden_size\n",
    "  3. num_layers\n",
    "  4. dropout\n",
    "  5. bidirectional\n",
    "  6. Note that it is not necessary to adjust other parameter values even though you are allowed to do so.\n",
    "<br><br>\n",
    "- #### Submit one zip file, named __lab-no3-yourname.zip__, that contains your Jupyter Notebook files and data files (e.g., input data and model files) through Turnitin on the class website. \n",
    "\n",
    "- #### The Jupyter notebook file must show all output results of your solution code. So please make sure that you run all the cells in the notebook file before your submission. Also note that Turnitin does not allow you to resubmit your lab assignment file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "120px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": "5",
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
