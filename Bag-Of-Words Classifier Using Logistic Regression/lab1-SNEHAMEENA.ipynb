{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification using Logistic Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise:\n",
    "\n",
    "1. Build a simple sentiment classifier with a new data given below, and predict test_data: utilize BoWClassifier2() defined in previous steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALGORITHM STEPS:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Step 1: Define the training Data Set and Testing Data Set\n",
    "Step 2: Clean up the data. (Since the data is already splitted here; no special mechanism or format is applied to clean the data)\n",
    "Step 3: Make the data to be all lower cases/ upper cases and if there are any restriction/rules to be applied; here since the data is small no rules have been applied\n",
    "Step 4: Build BOWClassifier with Sigmoid Activation Method; Here I have take relu(Recitifed Linear Unit) for affine map\n",
    "Step 5: Train the data and predict the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE ANALYSIS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 1: # Data Setup\n",
    "training_data = [(\"Well done\".split(), \"postive\"),\n",
    "        (\"Good work\".split(), \"postive\"),\n",
    "        (\"Great effort\".split(), \"postive\"),\n",
    "        (\"Weak\".split(), \"negative\"),\n",
    "        (\"Poor effort\".split(), \"negative\"),\n",
    "        (\"not good\".split(), \"negative\")]\n",
    "\n",
    "test_data = [(\"Great work\".split(), \"postive\"),\n",
    "             (\"Good job\".split(), \"postive\"),\n",
    "             (\"poor work\".split(), \"negative\"),\n",
    "             (\"not great\".split(), \"negative\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Well': 0, 'done': 1, 'Good': 2, 'work': 3, 'Great': 4, 'effort': 5, 'Weak': 6, 'Poor': 7, 'not': 8, 'good': 9, 'job': 10, 'poor': 11, 'great': 12}\n"
     ]
    }
   ],
   "source": [
    "# STEP 2 & STEP3: # Clean up of Data to collect distinct elements into bag.\n",
    "word_to_ix = {}\n",
    "for sent, _ in training_data + test_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "print(word_to_ix)\n",
    "\n",
    "label_to_ix = {\"negative\": 0, \"postive\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.2209,  0.2453,  0.0927,  0.1888, -0.2073,  0.0321, -0.0741, -0.1782,\n",
      "         -0.0025,  0.0384,  0.1747, -0.1245, -0.2363]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.0850], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# STEP 4:\n",
    "# In this Sentiment Analysis, we have 1 input of 2 classifiers are \"positive\" & \"negative\"; \n",
    "# One neuron and One Sigmoid Function as \"Activation Method\"\n",
    "\n",
    "# Collect word size from bag\n",
    "VOCAB_SIZE = len(word_to_ix)\n",
    "OUTPUT_SIZE = 1 \n",
    "class BoWClassifier2(nn.Module):  # inheriting from nn.Module!\n",
    "\n",
    "    def __init__(self, output_size, vocab_size):\n",
    "        \n",
    "        #input stage\n",
    "        super(BoWClassifier2, self).__init__() #If this super class isnt specified, then the classifier from nn.Module will be taken; To avoid that we are utilizing this one.\n",
    "    \n",
    "        # Neuron Stage; here its 1 bias\n",
    "        self.linear = nn.Linear(vocab_size, output_size)  # output_size = 1 #Rectified Linear Unit(reLu is used)\n",
    "        \n",
    "        #Activation Stage; and then it results in output\n",
    "    def forward(self, bow_vec):\n",
    "        return torch.sigmoid(self.linear(bow_vec))  #Sigmoid\n",
    "\n",
    "def make_bow_vector(sentence, word_to_ix):\n",
    "    vec = torch.zeros(len(word_to_ix))\n",
    "    for word in sentence:\n",
    "        vec[word_to_ix[word]] += 1\n",
    "    return vec.view(1, -1)    # return a matrix: 1 x len(vec)\n",
    "\n",
    "\n",
    "def make_target(label, label_to_ix):\n",
    "    return torch.LongTensor([label_to_ix[label]])  # [0] or [1]\n",
    "\n",
    "model = BoWClassifier2(OUTPUT_SIZE, VOCAB_SIZE)\n",
    "\n",
    "for param in model.parameters():\n",
    "    print(param)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5166]])\n",
      "tensor([[0.5872]])\n",
      "tensor([[0.5373]])\n",
      "tensor([[0.4616]])\n",
      "*******************\n",
      "Parameter containing:\n",
      "tensor([[ 0.2209,  0.2453,  0.0927,  0.1888, -0.2073,  0.0321, -0.0741, -0.1782,\n",
      "         -0.0025,  0.0384,  0.1747, -0.1245, -0.2363]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Sample Run on test data:\n",
    "with torch.no_grad():\n",
    "    for instance, label in test_data:   # Data Set: [\"Great work\", \"Good job\", \"poor work\", \"not great\"]\n",
    "        bow_vec = make_bow_vector(instance, word_to_ix)\n",
    "        output_probs = model(bow_vec)\n",
    "        print(output_probs)             # tensor equivalent for each Data Set.\n",
    "\n",
    "print(\"*******************\")\n",
    "print(next(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********************\n",
      "tensor([[0.9688]])\n",
      "***********************\n",
      "tensor([[0.8000]])\n",
      "***********************\n",
      "tensor([[0.7655]])\n",
      "***********************\n",
      "tensor([[0.1280]])\n",
      "Parameter containing:\n",
      "tensor([[ 1.5585,  1.5829,  1.5076,  1.6036,  2.1288,  0.2876, -2.0790, -2.2589,\n",
      "         -1.3865, -1.3456,  0.1747, -0.1245, -0.2363]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# STEP5 :CALCULATE THE LOSS FUNCTION\n",
    "loss_function = nn.BCELoss() \n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Pass over the training Set; Can assume to run epoch 30-50 times till the curve is flat; here its ran for 100 times\n",
    "for epoch in range(100):\n",
    "    for instance, label in training_data:\n",
    "        model.zero_grad() \n",
    "\n",
    "        bow_vec = make_bow_vector(instance, word_to_ix)    #Preparing the bow vector\n",
    "        target = make_target(label, label_to_ix).float().view(1,-1) \n",
    "        \n",
    "        output_probs = model(bow_vec)\n",
    "        \n",
    "        #Computing the loss and gradients\n",
    "        loss = loss_function(output_probs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for instance, label in test_data:  # Test_Data Set\n",
    "        bow_vec = make_bow_vector(instance, word_to_ix)\n",
    "        output_probs = model(bow_vec)\n",
    "        print(\"***********************\")\n",
    "        print(output_probs)\n",
    "\n",
    "print(next(model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SUMMARY \n",
    "1. Workflow Steps: Input-->Linear Layer --> Sigmoid Layer --> Output\n",
    "2. Loss Calculation Methodolgy: BCE Function\n",
    "3. Optimizer Used: SGD\n",
    "4. Learning Rate : 0.1\n",
    "5. Epoc: 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFERENCES\n",
    "1. Positive sentence's probability is higher 1st and 2nd Test_Data Sentences.\n",
    "2. Negative sentence's probability is lower than +ve sentences in 3rd and 4th Test_Data Sentences.\n",
    "3. A peculiar inference is the third data (\"(\"poor work\".split(), \"negative\")\") has the word \"poor\" in negative label and \"work\" under positive label.And hence the classifier has predicted it in middle level but as still negative with a little higher probability.\n",
    "4. Since the \"data_Set\" is small, loss function keeps little deviating and sometimes \"100\" epoch makes system over-trained as well.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
